{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "1. https://arxiv.org/pdf/2010.11929\n",
    "2. https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformer import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.cifar import CIFAR10, CIFAR100\n",
    "from torchvision.datasets.FashionMNIST import FashnionMNIST\n",
    "from torchvision.datasets.mnist import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST data\n",
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST data\n",
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining model and training options\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "# model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "# N_EPOCHS = 5\n",
    "# LR = 0.005\n",
    "\n",
    "# # Training loop\n",
    "# optimizer = Adam(model.parameters(), lr=LR)\n",
    "# criterion = CrossEntropyLoss()\n",
    "# for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "#     train_loss = 0.0\n",
    "#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "#         x, y = batch\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         y_hat = model(x)\n",
    "#         loss = criterion(y_hat, y)\n",
    "\n",
    "#         train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "# # Test loop\n",
    "# with torch.no_grad():\n",
    "#     correct, total = 0, 0\n",
    "#     test_loss = 0.0\n",
    "#     for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "#         x, y = batch\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         y_hat = model(x)\n",
    "#         loss = criterion(y_hat, y)\n",
    "#         test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "#         correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "#         total += len(x)\n",
    "#     print(f\"Test loss: {test_loss:.2f}\")\n",
    "#     print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation (Abdul's Implementation)\n",
    "- Train on CIFAR dataset (~76%)\n",
    "- Fine tuned on Fashion MNIST dataset (~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 4060 Laptop GPU)\n"
     ]
    }
   ],
   "source": [
    "# Defining model and training options\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\n",
    "    \"Using device: \",\n",
    "    device,\n",
    "    f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
    ")\n",
    "\n",
    "model = MyViT(\n",
    "    (1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10\n",
    ").to(device)\n",
    "N_EPOCHS = 25\n",
    "LR = 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [04:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     10\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, y)\n\u001b[1;32m     14\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cs444/vision_transformer_pytorch/vision_transformer.py:162\u001b[0m, in \u001b[0;36mMyViT.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Dividing images into patches\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     n, c, h, w \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 162\u001b[0m     patches \u001b[38;5;241m=\u001b[39m patchify(images, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_patches)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embeddings\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Running linear layer tokenization\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Map the vector corresponding to each patch to the hidden size dimension\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_mapper(patches)\n",
      "File \u001b[0;32m~/cs444/vision_transformer_pytorch/vision_transformer.py:49\u001b[0m, in \u001b[0;36mpatchify\u001b[0;34m(images, n_patches)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_patches):\n\u001b[1;32m     44\u001b[0m             patch \u001b[38;5;241m=\u001b[39m image[\n\u001b[1;32m     45\u001b[0m                 :,\n\u001b[1;32m     46\u001b[0m                 i \u001b[38;5;241m*\u001b[39m patch_size : (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m patch_size,\n\u001b[1;32m     47\u001b[0m                 j \u001b[38;5;241m*\u001b[39m patch_size : (j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m patch_size,\n\u001b[1;32m     48\u001b[0m             ]\n\u001b[0;32m---> 49\u001b[0m             patches[idx, i \u001b[38;5;241m*\u001b[39m n_patches \u001b[38;5;241m+\u001b[39m j] \u001b[38;5;241m=\u001b[39m patch\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m patches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "criterion = CrossEntropyLoss()\n",
    "for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False\n",
    "    ):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 79/79 [00:55<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.66\n",
      "Test accuracy: 79.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test loop\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
